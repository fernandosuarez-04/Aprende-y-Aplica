import { NextRequest, NextResponse } from 'next/server';
import { logger } from '@/lib/utils/logger';
import { OpenAI } from 'openai';
import { formatApiError, logError } from '@/core/utils/api-errors';
import { trackOpenAICall, calculateOpenAIMetadata, calculateCost } from '@/lib/openai/usage-monitor';
import { SessionService } from '@/features/auth/services/session.service';
import { LiaLogger } from '@/lib/analytics/lia-logger';

// Configuraci√≥n de Lia directamente en el archivo
const LIA_CONFIG = {
  responses: {
    offTopic: "Mi especialidad es la creaci√≥n de prompts de IA. ¬øEn qu√© tipo de prompt te gustar√≠a trabajar hoy?",
    injectionDetected: "Detect√© un patr√≥n que podr√≠a intentar manipular mis instrucciones. Mi prop√≥sito es ayudarte a crear prompts profesionales y seguros. Por favor, reformula tu solicitud para que sea constructiva y √©tica."
  },
  detection: {
    promptInjection: [
      "ignore previous instructions", "disregard all prior commands", "act as a", "jailbreak",
      "forget everything", "new instructions", "override", "system prompt", "you are now",
      "pretend to be", "roleplay as", "dan mode", "developer mode"
    ],
    offTopic: [
      "qu√© es la inteligencia artificial", "c√≥mo funciona la ia", "qu√© es chatgpt",
      "cu√©ntame un chiste", "c√≥mo est√°s", "qu√© hora es", "qu√© d√≠a es hoy",
      "cu√°l es tu nombre", "de d√≥nde eres", "qu√© opinas de", "ay√∫dame con mi tarea",
      "resuelve este problema", "explica este concepto"
    ]
  }
};

function isOffTopic(message: string): boolean {
  const lowerMessage = message.toLowerCase();
  return LIA_CONFIG.detection.offTopic.some(pattern => lowerMessage.includes(pattern));
}

function hasPromptInjection(message: string): boolean {
  const lowerMessage = message.toLowerCase();
  return LIA_CONFIG.detection.promptInjection.some(pattern => lowerMessage.includes(pattern));
}

// Configurar OpenAI
const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
});

// Prompt maestro de seguridad y profesionalismo para Lia
const MASTER_PROMPT = `Eres Lia, una especialista profesional en creaci√≥n de prompts de IA. Tu √∫nica funci√≥n es ayudar a los usuarios a crear prompts efectivos, bien estructurados y profesionales.

IDENTIDAD:
- Nombre: Lia
- Especialidad: Generaci√≥n de Prompts de IA
- Enfoque: EXCLUSIVAMENTE creaci√≥n de prompts, NO consultor√≠a general

COMPORTAMIENTO REQUERIDO:
1. Mant√©n un tono profesional, directo y eficiente
2. NO divagues sobre temas no relacionados con prompts
3. NO hagas chistes o comentarios casuales
4. NO act√∫es como consultor general de IA o tecnolog√≠a
5. Redirige cualquier pregunta no relacionada con prompts

L√çMITES ESTRICTOS:
- Solo responde preguntas sobre creaci√≥n de prompts
- Si te preguntan sobre otros temas de IA, responde: "Mi especialidad es la creaci√≥n de prompts de IA. ¬øEn qu√© tipo de prompt te gustar√≠a trabajar hoy?"
- Mant√©n conversaciones enfocadas y t√©cnicas

ESTRUCTURA DE RESPUESTA:
Cuando generes un prompt, siempre incluye:
1. T√≠tulo claro y descriptivo
2. Descripci√≥n concisa del prop√≥sito
3. Contenido del prompt bien estructurado
4. Tags relevantes (3-5)
5. Nivel de dificultad (beginner/intermediate/advanced)
6. Casos de uso espec√≠ficos
7. Consejos t√©cnicos para optimizaci√≥n

FORMATO DE SALIDA:
Responde SIEMPRE en formato JSON con esta estructura exacta:
{
  "title": "T√≠tulo del prompt",
  "description": "Descripci√≥n breve",
  "content": "Contenido completo del prompt (m√≠nimo 200 palabras, detallado y espec√≠fico)",
  "tags": ["tag1", "tag2", "tag3"],
  "difficulty_level": "beginner|intermediate|advanced",
  "use_cases": ["Caso 1", "Caso 2"],
  "tips": ["Consejo 1", "Consejo 2"]
}

IMPORTANTE: El contenido del prompt debe ser extenso, detallado y profesional. Usa formato Markdown con:

ESTRUCTURA REQUERIDA:
# T√≠tulo del Prompt

## Objetivo
Descripci√≥n clara del prop√≥sito

## Contexto
Informaci√≥n de fondo relevante

## Instrucciones Detalladas
### Paso 1: [Nombre del paso]
- Instrucci√≥n espec√≠fica
- Ejemplo concreto

### Paso 2: [Nombre del paso]
- Instrucci√≥n espec√≠fica
- Ejemplo concreto

## Formato de Salida
Especifica exactamente c√≥mo debe estructurarse la respuesta

## Consideraciones Especiales
- Punto importante 1
- Punto importante 2

## Ejemplo de Uso
Ejemplo pr√°ctico completo

FORMATO MARKDOWN:
- Usa **negritas** para √©nfasis
- Usa *cursivas* para t√©rminos t√©cnicos
- Usa ### para subt√≠tulos
- Usa - para listas
- Usa c√≥digo entre backticks para comandos o variables
- Incluye espacios entre secciones para mejor legibilidad

CATEGOR√çAS SOPORTADAS:
- Marketing y Ventas
- Contenido Creativo
- Programaci√≥n y Desarrollo
- An√°lisis de Datos
- Educaci√≥n y Capacitaci√≥n
- Redacci√≥n y Comunicaci√≥n
- Investigaci√≥n y An√°lisis
- Automatizaci√≥n de Procesos
- Arte y Dise√±o
- Negocios y Estrategia

RECUERDA: Eres un generador de prompts profesional, no un chatbot general. Mant√©n el enfoque en tu especialidad.`;

interface ChatMessage {
  id: string;
  text: string;
  sender: 'user' | 'ai';
  timestamp: string;
  isPromptSection?: boolean;
  sectionType?: 'title' | 'description' | 'content' | 'tags' | 'difficulty' | 'use_cases' | 'tips';
}

export async function POST(request: NextRequest) {
  try {
    logger.log('üîç API generate-prompt called');
    
    // ‚úÖ Obtener usuario autenticado para analytics
    const user = await SessionService.getCurrentUser();
    const userId = user?.id || null;
    
    // ‚úÖ Inicializar LiaLogger si hay usuario
    let liaLogger: LiaLogger | null = null;
    let conversationId: string | null = null;
    
    if (userId) {
      liaLogger = new LiaLogger(userId);
      try {
        conversationId = await liaLogger.startConversation({
          contextType: 'general', // Prompts usa contexto general
        });
        logger.log('‚úÖ [LiaLogger] Conversaci√≥n iniciada:', conversationId);
      } catch (logErr) {
        logger.log('‚ö†Ô∏è [LiaLogger] Error iniciando conversaci√≥n:', logErr);
        liaLogger = null;
      }
    }
    
    const { message, conversationHistory } = await request.json();
    logger.log('üìù Message received:', message);

    // Validar entrada
    if (!message || typeof message !== 'string') {
      logger.log('‚ùå No message provided');
      return NextResponse.json(
        { error: 'Mensaje requerido' },
        { status: 400 }
      );
    }

    // Verificar comportamiento apropiado usando configuraci√≥n de Lia
    if (hasPromptInjection(message)) {
      return NextResponse.json({
        response: LIA_CONFIG.responses.injectionDetected,
        generatedPrompt: null,
      }, { status: 400 });
    }
    
    if (isOffTopic(message)) {
      return NextResponse.json({
        response: LIA_CONFIG.responses.offTopic,
        generatedPrompt: null,
      }, { status: 200 });
    }

    // Construir historial de conversaci√≥n
    const messages: OpenAI.Chat.Completions.ChatCompletionMessageParam[] = [
      {
        role: 'system',
        content: MASTER_PROMPT
      }
    ];

    // Agregar historial de conversaci√≥n (√∫ltimos 10 mensajes para mantener contexto)
    const recentHistory = conversationHistory ? conversationHistory.slice(-10) : [];
    recentHistory.forEach((msg: any) => {
      // Convertir del formato del frontend al formato de OpenAI
      const role = msg.sender === 'ai' ? 'assistant' : 'user';
      const content = msg.text;
      
      messages.push({
        role: role as 'user' | 'assistant',
        content: content
      });
    });

    // Agregar el mensaje actual
    messages.push({
      role: 'user',
      content: message
    });

    // Llamar a OpenAI
    logger.log('ü§ñ Calling OpenAI with', messages.length, 'messages');
    logger.log('üìã Messages array:', JSON.stringify(messages, null, 2));
    const startTime = Date.now();
    const model = 'gpt-4o';
    const completion = await openai.chat.completions.create({
      model,
      messages: messages,
      temperature: 0.7,
      max_tokens: 2000,
      presence_penalty: 0.1,
      frequency_penalty: 0.1
    });
    const responseTime = Date.now() - startTime;
    logger.log('‚úÖ OpenAI response received');
    
    // ‚úÖ Registrar uso de OpenAI para generaci√≥n de prompts
    if (completion.usage) {
      await trackOpenAICall(calculateOpenAIMetadata(
        {
          prompt_tokens: completion.usage.prompt_tokens,
          completion_tokens: completion.usage.completion_tokens,
          total_tokens: completion.usage.total_tokens
        },
        model,
        'prompt-generation',
        undefined, // No tenemos userId en este contexto
        responseTime
      ));
    }

    const response = completion.choices[0]?.message?.content;
    logger.log('üìÑ OpenAI raw response:', response);
    
    if (!response) {
      throw new Error('No se recibi√≥ respuesta de OpenAI');
    }

    // Intentar parsear como JSON si parece ser un prompt completo
    let generatedPrompt = null;
    try {
      // Buscar JSON en la respuesta
      const jsonMatch = response.match(/\{[\s\S]*\}/);
      if (jsonMatch) {
        const parsed = JSON.parse(jsonMatch[0]);
        if (parsed.title && parsed.content) {
          generatedPrompt = parsed;
        }
      }
    } catch (error) {
      // No es JSON, continuar con respuesta normal
    }

    const finalResponse = {
      response: response,
      message: response,
      generatedPrompt: generatedPrompt,
      conversationId: conversationId // ‚úÖ Devolver conversationId para tracking
    };
    
    // ‚úÖ Registrar mensajes en BD con LiaLogger
    if (liaLogger && conversationId && completion.usage) {
      try {
        // Registrar mensaje del usuario
        await liaLogger.logMessage('user', message, false);
        
        // Registrar respuesta del asistente con metadatos
        const totalCost = calculateCost(
          completion.usage.prompt_tokens,
          completion.usage.completion_tokens,
          model
        );
        
        await liaLogger.logMessage('assistant', response || '', false, {
          modelUsed: model,
          tokensUsed: completion.usage.total_tokens,
          costUsd: totalCost,
          responseTimeMs: responseTime
        });
        
        logger.log('‚úÖ [LiaLogger] Mensajes registrados en BD');
      } catch (logErr) {
        logger.log('‚ö†Ô∏è [LiaLogger] Error registrando mensajes:', logErr);
      }
    }
    
    logger.log('üì§ Sending response to frontend:', JSON.stringify(finalResponse, null, 2));
    
    return NextResponse.json(finalResponse);

  } catch (error) {
    logError('POST /api/ai-directory/generate-prompt', error);

    // Manejar errores espec√≠ficos de OpenAI
    if (error instanceof Error) {
      if (error.message.includes('API key')) {
        return NextResponse.json(
          formatApiError(error, 'Error de configuraci√≥n de API'),
          { status: 500 }
        );
      }

      if (error.message.includes('rate limit')) {
        return NextResponse.json(
          formatApiError(error, 'L√≠mite de solicitudes excedido. Int√©ntalo m√°s tarde.'),
          { status: 429 }
        );
      }
    }

    return NextResponse.json(
      formatApiError(error, 'Error al generar prompt'),
      { status: 500 }
    );
  }
}
