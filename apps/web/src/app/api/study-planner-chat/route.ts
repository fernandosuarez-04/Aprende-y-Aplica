/**
 * API Route exclusiva para el Planificador de Estudios - LIA
 * 
 * Este endpoint est√° completamente separado del ai-chat general
 * para manejar de forma espec√≠fica las interacciones con el planificador.
 * 
 * Utiliza Gemini 2.0 Flash de Google directamente SIN filtros de prompt-leak.
 */

import { NextRequest, NextResponse } from 'next/server';
import { GoogleGenerativeAI, HarmCategory, HarmBlockThreshold } from '@google/generative-ai';

// Logger simple
const logger = {
    info: (...args: unknown[]) => console.log('[STUDY-PLANNER-API]', ...args),
    warn: (...args: unknown[]) => console.warn('[STUDY-PLANNER-API]', ...args),
    error: (...args: unknown[]) => console.error('[STUDY-PLANNER-API]', ...args)
};

// Inicializar cliente de Google Gemini
const googleApiKey = process.env.GOOGLE_API_KEY;
let genAI: GoogleGenerativeAI | null = null;

if (googleApiKey) {
    genAI = new GoogleGenerativeAI(googleApiKey);
    logger.info('‚úÖ Google Gemini API inicializada para Study Planner');
} else {
    logger.error('‚ùå GOOGLE_API_KEY no est√° configurada');
}

// Tipos
interface ChatMessage {
    role: 'user' | 'assistant' | 'system';
    content: string;
}

interface StudyPlannerRequest {
    message: string;
    conversationHistory?: ChatMessage[];
    systemPrompt: string;
    userId?: string;
    userName?: string;
}

/**
 * Handler principal para el chat del planificador de estudios
 */
export async function POST(request: NextRequest) {
    try {
        logger.info('üì• Recibida solicitud de Study Planner Chat');

        // Verificar que Gemini est√° disponible
        if (!genAI) {
            logger.error('Gemini API no est√° inicializada');
            return NextResponse.json(
                { error: 'Servicio de IA no disponible' },
                { status: 503 }
            );
        }

        // Parsear el body con manejo de errores
        let body: StudyPlannerRequest;
        try {
            body = await request.json() as StudyPlannerRequest;
        } catch (parseError) {
            logger.error('‚ùå Error parseando body de la solicitud:', parseError);
            return NextResponse.json(
                { error: 'Body de solicitud inv√°lido o vac√≠o' },
                { status: 400 }
            );
        }

        const { message, conversationHistory = [], systemPrompt, userId, userName } = body;

        logger.info('üìù Mensaje recibido:', message?.substring(0, 100));
        logger.info('üìö Historial de conversaci√≥n:', conversationHistory.length, 'mensajes');
        logger.info('üë§ Usuario:', userName || userId || 'An√≥nimo');

        if (!message) {
            return NextResponse.json(
                { error: 'Se requiere un mensaje' },
                { status: 400 }
            );
        }

        if (!systemPrompt) {
            return NextResponse.json(
                { error: 'Se requiere el prompt del sistema' },
                { status: 400 }
            );
        }

        // Configurar el modelo con safety settings relajados para el planificador
        const model = genAI.getGenerativeModel({
            model: 'gemini-2.0-flash',
            safetySettings: [
                {
                    category: HarmCategory.HARM_CATEGORY_HARASSMENT,
                    threshold: HarmBlockThreshold.BLOCK_NONE,
                },
                {
                    category: HarmCategory.HARM_CATEGORY_HATE_SPEECH,
                    threshold: HarmBlockThreshold.BLOCK_NONE,
                },
                {
                    category: HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,
                    threshold: HarmBlockThreshold.BLOCK_NONE,
                },
                {
                    category: HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,
                    threshold: HarmBlockThreshold.BLOCK_NONE,
                },
            ],
            generationConfig: {
                temperature: 0.7,
                maxOutputTokens: 4096,
                topP: 0.95,
                topK: 40,
            },
        });

        logger.info('üöÄ Iniciando chat con Gemini 2.0 Flash...');

        // Construir el historial para Gemini, filtrando mensajes vac√≠os o inv√°lidos
        const geminiHistory = conversationHistory
            .filter(msg => msg.content && msg.content.trim() !== '') // IMPORTANTE: Filtrar vac√≠os
            .map(msg => ({
                role: msg.role === 'assistant' ? 'model' as const : 'user' as const,
                parts: [{ text: msg.content }]
            }));

        // Log para debug
        if (geminiHistory.length > 0) {
            logger.info('üîç Primer mensaje del historial a enviar:', JSON.stringify(geminiHistory[0]).substring(0, 100));
        }

        // Iniciar chat con el historial
        const chat = model.startChat({
            history: [
                // A√±adir el system prompt como primer mensaje del modelo
                {
                    role: 'user',
                    parts: [{ text: 'Instrucciones del sistema para esta conversaci√≥n:' }]
                },
                {
                    role: 'model',
                    parts: [{ text: 'Entendido. He le√≠do y memorizado las siguientes instrucciones que seguir√© durante toda la conversaci√≥n.' }]
                },
                {
                    role: 'user',
                    parts: [{ text: systemPrompt }]
                },
                {
                    role: 'model',
                    parts: [{ text: 'Perfecto, he internalizado todas las instrucciones. Estoy listo para ayudar como LIA, el asistente del Planificador de Estudios. Responder√© en espa√±ol, de forma natural y amigable, siguiendo todas las reglas establecidas.' }]
                },
                // A√±adir el historial de conversaci√≥n real
                ...geminiHistory
            ],
        });

        // Enviar el mensaje y obtener la respuesta
        const result = await chat.sendMessage(message);
        const response = await result.response;

        // Manejar posible bug de Node.js con TransformStreams
        let responseText: string;
        try {
            responseText = response.text();
        } catch (textError) {
            logger.warn('‚ö†Ô∏è Error obteniendo texto con .text(), intentando alternativa:', textError);
            // Fallback: acceder directamente a los candidates
            const candidates = response.candidates;
            if (candidates && candidates.length > 0 && candidates[0].content?.parts?.[0]?.text) {
                responseText = candidates[0].content.parts[0].text;
            } else {
                throw new Error('No se pudo extraer el texto de la respuesta de Gemini');
            }
        }

        logger.info('‚úÖ Respuesta recibida de Gemini');
        logger.info('üìÑ Longitud de respuesta:', responseText.length, 'caracteres');
        logger.info('üìÑ Primeros 500 caracteres:', responseText.substring(0, 500));

        // ‚ö†Ô∏è NO aplicamos filtro aqu√≠ - queremos ver exactamente qu√© devuelve el modelo
        // Si hay problemas, los manejaremos ajustando el prompt, no filtrando

        return NextResponse.json({
            response: responseText,
            model: 'gemini-2.0-flash',
            timestamp: new Date().toISOString()
        });

    } catch (error) {
        logger.error('‚ùå Error en Study Planner Chat:', error);

        const errorMessage = error instanceof Error ? error.message : 'Error desconocido';

        return NextResponse.json(
            {
                error: 'Error al procesar la solicitud',
                details: errorMessage
            },
            { status: 500 }
        );
    }
}

/**
 * Endpoint GET para verificar que la API est√° funcionando
 */
export async function GET() {
    return NextResponse.json({
        status: 'ok',
        service: 'Study Planner Chat API',
        geminiAvailable: !!genAI,
        timestamp: new Date().toISOString()
    });
}
