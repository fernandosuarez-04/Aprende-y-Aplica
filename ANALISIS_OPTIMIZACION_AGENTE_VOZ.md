# üìä An√°lisis y Optimizaci√≥n del Agente de Voz con ElevenLabs

## üéØ Resumen Ejecutivo

El agente de voz (LIA Voice) utiliza una arquitectura de m√∫ltiples capas que, aunque robusta y completa, introduce latencias significativas en la interacci√≥n usuario-asistente. Este documento analiza el flujo actual y propone optimizaciones concretas para **reducir el tiempo de respuesta en un 40-60%**.

**Tiempo de respuesta actual estimado**: 3.5-6 segundos
**Tiempo de respuesta objetivo**: 1.5-2.5 segundos

---

## üîç An√°lisis del Flujo Actual

### Arquitectura Actual

```
Usuario (Habla)
    ‚Üì [Web Speech API - 500-1000ms]
Transcripci√≥n de Texto
    ‚Üì [HTTP Request - 50-100ms]
/api/lia/onboarding-chat (Middleware)
    ‚Üì [HTTP Request interno - 50-100ms]
/api/ai-chat (Motor Principal)
    ‚Üì [OpenAI API - 1000-2500ms]
Respuesta GPT-4o-mini
    ‚Üì [Analytics + DB writes - 200-400ms]
Respuesta al Frontend
    ‚Üì [ElevenLabs API - 800-1500ms]
Audio MP3 (Blob)
    ‚Üì [Descarga + Buffer - 100-300ms]
Reproducci√≥n de Audio
```

### Desglose de Latencia Total (Promedio)

| Componente | Tiempo Estimado | % del Total | Optimizable |
|-----------|----------------|-------------|-------------|
| **Reconocimiento de voz** | 500-1000ms | 15% | ‚ö†Ô∏è Limitado |
| **Request a /onboarding-chat** | 50-100ms | 2% | ‚úÖ Eliminable |
| **Request interno a /ai-chat** | 50-100ms | 2% | ‚úÖ Eliminable |
| **Procesamiento OpenAI** | 1000-2500ms | 45% | ‚úÖ Optimizable |
| **Analytics as√≠ncronos** | 200-400ms | 8% | ‚úÖ Ya optimizado |
| **Generaci√≥n de audio (ElevenLabs)** | 800-1500ms | 25% | ‚úÖ Muy optimizable |
| **Descarga y reproducci√≥n** | 100-300ms | 3% | ‚úÖ Optimizable |
| **TOTAL** | **3.5-6 segundos** | 100% | |

---

## üöÄ Propuestas de Optimizaci√≥n

### 1. üéØ **Eliminaci√≥n de Middleware Redundante** [PRIORIDAD ALTA]

**Problema**: El endpoint `/api/lia/onboarding-chat` solo act√∫a como proxy, agregando latencia innecesaria.

**Soluci√≥n**: Eliminar el middleware y llamar directamente a `/api/ai-chat` desde el frontend.

**Impacto**:
- ‚è±Ô∏è Reducci√≥n de latencia: **100-200ms**
- üéØ Simplicidad arquitect√≥nica
- üìä Menos puntos de fallo

**Implementaci√≥n**:

```typescript
// ANTES (OnboardingAgent.tsx - l√≠nea 481)
const response = await fetch('/api/lia/onboarding-chat', {
  method: 'POST',
  headers: { 'Content-Type': 'application/json' },
  body: JSON.stringify({ question, context, userName, pageContext }),
});

// DESPU√âS
const response = await fetch('/api/ai-chat', {
  method: 'POST',
  headers: { 'Content-Type': 'application/json' },
  body: JSON.stringify({
    message: question,
    context: 'onboarding',
    conversationHistory: conversationHistory || [],
    userName: userName,
    pageContext: pageContext,
    language: 'es'
  }),
});
```

**Trabajo requerido**: 5 minutos ‚è∞

---

### 2. ‚ö° **Streaming de Audio con ElevenLabs** [PRIORIDAD ALTA]

**Problema**: La generaci√≥n completa del audio antes de reproducirlo a√±ade latencia significativa (800-1500ms).

**Soluci√≥n**: Usar el endpoint de streaming de ElevenLabs para reproducir audio mientras se genera.

**Impacto**:
- ‚è±Ô∏è Reducci√≥n de latencia percibida: **600-1000ms**
- üéß Inicio de reproducci√≥n casi inmediato
- üíæ Menor uso de memoria

**Implementaci√≥n**:

```typescript
// apps/web/src/core/components/OnboardingAgent/OnboardingAgent.tsx
// L√≠nea 141 - Funci√≥n speakText

const speakText = async (text: string) => {
  if (!isAudioEnabled || typeof window === 'undefined') return;

  stopAllAudio();

  try {
    setIsSpeaking(true);

    const apiKey = 'sk_dd0d1757269405cd26d5e22fb14c54d2f49c4019fd8e86d0';
    const voiceId = '15Y62ZlO8it2f5wduybx';
    const modelId = 'eleven_multilingual_v2';

    if (!apiKey || !voiceId) {
      // Fallback a Web Speech API (c√≥digo existente)
      return;
    }

    const controller = new AbortController();
    ttsAbortRef.current = controller;

    // ‚úÖ OPTIMIZACI√ìN: Usar streaming endpoint
    const response = await fetch(
      `https://api.elevenlabs.io/v1/text-to-speech/${voiceId}/stream`, // ‚Üê endpoint streaming
      {
        signal: controller.signal,
        method: 'POST',
        headers: {
          'Accept': 'audio/mpeg',
          'Content-Type': 'application/json',
          'xi-api-key': apiKey,
        },
        body: JSON.stringify({
          text: text,
          model_id: modelId || 'eleven_multilingual_v2',
          voice_settings: {
            stability: 0.5,
            similarity_boost: 0.75,
            style: 0.5,
            use_speaker_boost: true
          },
          // ‚úÖ Configuraci√≥n optimizada para latencia
          optimize_streaming_latency: 4, // 0-4, siendo 4 la m√°s r√°pida
          output_format: 'mp3_22050_32' // Menor calidad = menor latencia
        }),
      }
    );

    if (!response.ok) {
      throw new Error(`ElevenLabs API error: ${response.status}`);
    }

    // ‚úÖ OPTIMIZACI√ìN: Streaming con MediaSource
    if (response.body) {
      const mediaSource = new MediaSource();
      const audioUrl = URL.createObjectURL(mediaSource);
      const audio = new Audio(audioUrl);
      audioRef.current = audio;

      mediaSource.addEventListener('sourceopen', async () => {
        const sourceBuffer = mediaSource.addSourceBuffer('audio/mpeg');
        const reader = response.body!.getReader();

        // Funci√≥n para leer chunks y alimentar el buffer
        const pump = async () => {
          try {
            const { done, value } = await reader.read();

            if (done) {
              if (mediaSource.readyState === 'open') {
                mediaSource.endOfStream();
              }
              return;
            }

            // Esperar a que el buffer est√© listo antes de agregar m√°s datos
            if (sourceBuffer.updating) {
              await new Promise(resolve => {
                sourceBuffer.addEventListener('updateend', resolve, { once: true });
              });
            }

            sourceBuffer.appendBuffer(value);

            // Iniciar reproducci√≥n cuando tengamos suficiente buffer
            if (audio.paused && sourceBuffer.buffered.length > 0) {
              try {
                await audio.play();
              } catch (playError) {
                console.warn('‚ö†Ô∏è Autoplay bloqueado:', playError);
              }
            }

            pump();
          } catch (error) {
            if (error instanceof DOMException && error.name === 'AbortError') {
              console.log('Streaming abortado');
            } else {
              console.error('Error en streaming:', error);
            }
          }
        };

        pump();
      });

      audio.onended = () => {
        setIsSpeaking(false);
        URL.revokeObjectURL(audioUrl);
        if (audioRef.current === audio) audioRef.current = null;
      };

      audio.onerror = () => {
        setIsSpeaking(false);
        URL.revokeObjectURL(audioUrl);
        if (audioRef.current === audio) audioRef.current = null;
      };

      // Clear abort controller after setup
      if (ttsAbortRef.current === controller) ttsAbortRef.current = null;
    }
  } catch (error: any) {
    if (error && (error.name === 'AbortError' || error.message?.includes('aborted'))) {
      console.log('TTS abortado:', error.message || error);
    } else {
      console.error('Error en s√≠ntesis de voz con ElevenLabs:', error);
    }
    setIsSpeaking(false);
  }
};
```

**Trabajo requerido**: 30 minutos ‚è∞

**Nota**: MediaSource puede tener soporte limitado en algunos navegadores. Incluir fallback al m√©todo actual.

---

### 3. üß† **Optimizaci√≥n del Modelo OpenAI** [PRIORIDAD MEDIA]

**Problema**: GPT-4o-mini, aunque r√°pido, puede optimizarse a√∫n m√°s con configuraci√≥n espec√≠fica para conversaci√≥n por voz.

**Soluci√≥n**: Ajustar par√°metros del modelo para respuestas m√°s concisas y r√°pidas.

**Impacto**:
- ‚è±Ô∏è Reducci√≥n de latencia: **200-500ms**
- üí∞ Reducci√≥n de costos (menos tokens)
- üéØ Respuestas m√°s naturales para voz

**Implementaci√≥n**:

```typescript
// apps/web/src/app/api/ai-chat/route.ts
// L√≠nea 1067 - Configuraci√≥n del modelo

body: JSON.stringify({
  model: process.env.CHATBOT_MODEL || 'gpt-4o-mini',
  messages: messages,
  // ‚úÖ OPTIMIZACI√ìN: Configuraci√≥n para conversaci√≥n por voz
  temperature: context === 'onboarding' ? 0.7 : parseFloat(process.env.CHATBOT_TEMPERATURE || '0.6'),
  max_tokens: context === 'onboarding'
    ? 150  // ‚Üê Respuestas m√°s cortas para voz (era 500-1000)
    : parseInt(process.env.CHATBOT_MAX_TOKENS || '500'),
  stream: false,
  // ‚úÖ Nuevos par√°metros de optimizaci√≥n
  presence_penalty: 0.6, // Reducir repeticiones
  frequency_penalty: 0.3, // Variar vocabulario
  top_p: 0.9, // M√°s determin√≠stico
}),
```

**Modificar el prompt del sistema para onboarding**:

```typescript
// apps/web/src/app/api/ai-chat/route.ts
// En getContextPrompt, agregar instrucci√≥n espec√≠fica para onboarding

if (context === 'onboarding') {
  return `...

  IMPORTANTE - FORMATO PARA VOZ:
  - Respuestas M√ÅXIMO 2-3 oraciones (50-80 palabras)
  - Lenguaje conversacional y natural
  - Sin listas largas ni explicaciones extensas
  - Directo al punto, como si estuvieras hablando
  - Usa un tono entusiasta y amigable

  ...`;
}
```

**Trabajo requerido**: 15 minutos ‚è∞

---

### 4. üé§ **Prefetch de Respuestas Predictivas** [PRIORIDAD BAJA - AVANZADA]

**Problema**: Cada pregunta requiere esperar el procesamiento completo.

**Soluci√≥n**: Cachear respuestas a preguntas frecuentes del onboarding.

**Impacto**:
- ‚è±Ô∏è Reducci√≥n de latencia para preguntas comunes: **2000-3500ms** (casi instant√°neo)
- üí∞ Reducci√≥n de costos significativa
- üéØ Experiencia ultra-r√°pida para casos comunes

**Implementaci√≥n**:

```typescript
// apps/web/src/core/components/OnboardingAgent/cache.ts (nuevo archivo)

interface CachedResponse {
  text: string;
  audioUrl: string; // Audio pregenerado
  timestamp: number;
}

const RESPONSE_CACHE = new Map<string, CachedResponse>();

// Preguntas frecuentes del onboarding
const COMMON_QUESTIONS = [
  "¬øQu√© tipo de cursos tienen?",
  "¬øC√≥mo funciona la plataforma?",
  "¬øPuedes ayudarme con tareas?",
  "¬øQu√© es la inteligencia artificial?",
  "¬øHay proyectos pr√°cticos?",
  "¬øCu√°nto cuesta?",
  "¬øPuedo probar antes de pagar?",
];

// Funci√≥n para normalizar preguntas
function normalizeQuestion(question: string): string {
  return question
    .toLowerCase()
    .trim()
    .replace(/[¬ø?¬°!.,;]/g, '')
    .replace(/\s+/g, ' ');
}

// Funci√≥n para calcular similitud (simple)
function calculateSimilarity(q1: string, q2: string): number {
  const words1 = normalizeQuestion(q1).split(' ');
  const words2 = normalizeQuestion(q2).split(' ');

  const commonWords = words1.filter(w => words2.includes(w)).length;
  const totalWords = Math.max(words1.length, words2.length);

  return commonWords / totalWords;
}

// Funci√≥n para buscar en cach√©
export function getCachedResponse(question: string): CachedResponse | null {
  const normalized = normalizeQuestion(question);

  // B√∫squeda exacta
  if (RESPONSE_CACHE.has(normalized)) {
    return RESPONSE_CACHE.get(normalized)!;
  }

  // B√∫squeda por similitud (>80%)
  for (const [cachedQ, response] of RESPONSE_CACHE.entries()) {
    const similarity = calculateSimilarity(question, cachedQ);
    if (similarity > 0.8) {
      console.log(`‚úÖ Cache hit con similitud ${(similarity * 100).toFixed(0)}%`);
      return response;
    }
  }

  return null;
}

// Funci√≥n para agregar a cach√©
export function cacheResponse(question: string, text: string, audioUrl: string) {
  const normalized = normalizeQuestion(question);
  RESPONSE_CACHE.set(normalized, {
    text,
    audioUrl,
    timestamp: Date.now()
  });
}

// Funci√≥n para precargar respuestas comunes
export async function preloadCommonResponses() {
  console.log('üîÑ Precargando respuestas comunes del onboarding...');

  for (const question of COMMON_QUESTIONS) {
    try {
      // Generar respuesta con IA
      const response = await fetch('/api/ai-chat', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          message: question,
          context: 'onboarding',
          language: 'es'
        }),
      });

      const data = await response.json();
      const text = data.response;

      // Generar audio con ElevenLabs
      const audioBlob = await generateAudio(text);
      const audioUrl = URL.createObjectURL(audioBlob);

      // Cachear
      cacheResponse(question, text, audioUrl);

      console.log(`‚úÖ Respuesta cacheada: "${question}"`);
    } catch (error) {
      console.error(`‚ùå Error cacheando respuesta para: "${question}"`, error);
    }
  }

  console.log(`‚úÖ ${RESPONSE_CACHE.size} respuestas cacheadas`);
}

async function generateAudio(text: string): Promise<Blob> {
  const apiKey = 'sk_dd0d1757269405cd26d5e22fb14c54d2f49c4019fd8e86d0';
  const voiceId = '15Y62ZlO8it2f5wduybx';

  const response = await fetch(
    `https://api.elevenlabs.io/v1/text-to-speech/${voiceId}`,
    {
      method: 'POST',
      headers: {
        'Accept': 'audio/mpeg',
        'Content-Type': 'application/json',
        'xi-api-key': apiKey,
      },
      body: JSON.stringify({
        text: text,
        model_id: 'eleven_multilingual_v2',
        voice_settings: {
          stability: 0.5,
          similarity_boost: 0.75,
          style: 0.5,
          use_speaker_boost: true
        }
      }),
    }
  );

  return await response.blob();
}
```

**Uso en OnboardingAgent.tsx**:

```typescript
import { getCachedResponse, cacheResponse, preloadCommonResponses } from './cache';

// Al montar el componente
useEffect(() => {
  // Precargar respuestas comunes en background
  preloadCommonResponses().catch(console.error);
}, []);

// En handleVoiceQuestion
const handleVoiceQuestion = async (question: string) => {
  if (!question.trim()) return;

  // Intentar obtener de cach√©
  const cached = getCachedResponse(question);

  if (cached) {
    console.log('‚ö° Usando respuesta cacheada');

    // Usar respuesta y audio precacheados
    setConversationHistory(prev => [
      ...prev,
      { role: 'user', content: question },
      { role: 'assistant', content: cached.text }
    ]);

    // Reproducir audio precargado
    const audio = new Audio(cached.audioUrl);
    audioRef.current = audio;
    audio.play();

    return;
  }

  // Si no hay cach√©, proceder normalmente
  try {
    const response = await fetch('/api/ai-chat', { /* ... */ });
    const data = await response.json();

    // Cachear para futuras consultas
    const audioBlob = await generateAudioBlob(data.response);
    const audioUrl = URL.createObjectURL(audioBlob);
    cacheResponse(question, data.response, audioUrl);

    // Reproducir respuesta
    await speakText(data.response);
  } catch (error) {
    console.error('Error:', error);
  }
};
```

**Trabajo requerido**: 2 horas ‚è∞

---

### 5. üé® **Feedback Visual Inmediato** [PRIORIDAD MEDIA]

**Problema**: El usuario no tiene retroalimentaci√≥n inmediata de que su pregunta est√° siendo procesada.

**Soluci√≥n**: Mostrar indicadores visuales y sonoros instant√°neos.

**Impacto**:
- üéØ Mejor UX percibida
- üòä Menor frustraci√≥n del usuario
- ‚è±Ô∏è Sensaci√≥n de respuesta m√°s r√°pida

**Implementaci√≥n**:

```typescript
// apps/web/src/core/components/OnboardingAgent/OnboardingAgent.tsx

const handleVoiceQuestion = async (question: string) => {
  if (!question.trim()) return;

  processingRef.current = true;
  setIsProcessing(true);

  // ‚úÖ FEEDBACK INMEDIATO: Sonido de confirmaci√≥n
  playConfirmationSound(); // Nuevo: beep suave

  // ‚úÖ FEEDBACK INMEDIATO: Mostrar mensaje "Entendido..." mientras procesa
  setConversationHistory(prev => [
    ...prev,
    { role: 'user', content: question },
    { role: 'assistant', content: 'üí≠ Pensando...' } // Placeholder temporal
  ]);

  try {
    const response = await fetch('/api/ai-chat', { /* ... */ });
    const data = await response.json();

    // ‚úÖ Reemplazar placeholder con respuesta real
    setConversationHistory(prev => {
      const newHistory = [...prev];
      newHistory[newHistory.length - 1] = {
        role: 'assistant',
        content: data.response
      };
      return newHistory;
    });

    await speakText(data.response);
  } catch (error) {
    console.error('Error:', error);
  } finally {
    processingRef.current = false;
    setIsProcessing(false);
  }
};

// Funci√≥n para sonido de confirmaci√≥n
function playConfirmationSound() {
  const audioContext = new AudioContext();
  const oscillator = audioContext.createOscillator();
  const gainNode = audioContext.createGain();

  oscillator.connect(gainNode);
  gainNode.connect(audioContext.destination);

  oscillator.frequency.value = 800; // Tono agudo
  gainNode.gain.value = 0.1; // Volumen bajo

  oscillator.start();
  oscillator.stop(audioContext.currentTime + 0.1); // 100ms
}
```

**Trabajo requerido**: 30 minutos ‚è∞

---

### 6. üîß **Configuraci√≥n Optimizada de ElevenLabs** [PRIORIDAD MEDIA]

**Problema**: Los par√°metros actuales de ElevenLabs priorizan calidad sobre velocidad.

**Soluci√≥n**: Ajustar configuraci√≥n para balance calidad/velocidad √≥ptimo.

**Impacto**:
- ‚è±Ô∏è Reducci√≥n de latencia: **200-400ms**
- üéß Calidad de audio suficiente para conversaci√≥n
- üí∞ Posible reducci√≥n de costos

**Implementaci√≥n**:

```typescript
// apps/web/src/core/components/OnboardingAgent/OnboardingAgent.tsx
// L√≠nea 201 - Configuraci√≥n de ElevenLabs

body: JSON.stringify({
  text: text,
  model_id: 'eleven_turbo_v2', // ‚úÖ Modelo turbo (m√°s r√°pido que multilingual_v2)
  voice_settings: {
    stability: 0.4,            // ‚¨áÔ∏è Reducido de 0.5 para m√°s velocidad
    similarity_boost: 0.65,    // ‚¨áÔ∏è Reducido de 0.75
    style: 0.3,                // ‚¨áÔ∏è Reducido de 0.5
    use_speaker_boost: false   // ‚úÖ Desactivado para mayor velocidad
  },
  // ‚úÖ NUEVOS par√°metros de optimizaci√≥n
  optimize_streaming_latency: 4, // M√°xima optimizaci√≥n (0-4)
  output_format: 'mp3_22050_32' // Menor bitrate = menor latencia
})
```

**Modelos de ElevenLabs por velocidad**:

| Modelo | Calidad | Velocidad | Recomendaci√≥n |
|--------|---------|-----------|---------------|
| `eleven_multilingual_v2` | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚è±Ô∏è‚è±Ô∏è | Actual (lento) |
| `eleven_turbo_v2` | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚è±Ô∏è‚è±Ô∏è‚è±Ô∏è‚è±Ô∏è | **Recomendado** |
| `eleven_turbo_v2_5` | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚è±Ô∏è‚è±Ô∏è‚è±Ô∏è‚è±Ô∏è | Mejor opci√≥n |

**Trabajo requerido**: 5 minutos ‚è∞

---

### 7. üîÑ **Paralelizaci√≥n de Operaciones** [PRIORIDAD ALTA]

**Problema**: Algunas operaciones se ejecutan secuencialmente cuando podr√≠an ser paralelas.

**Soluci√≥n**: Ejecutar analytics y otras operaciones no cr√≠ticas en paralelo.

**Impacto**:
- ‚è±Ô∏è Reducci√≥n de latencia percibida: **100-200ms**
- üéØ Mejor utilizaci√≥n de recursos

**Implementaci√≥n**:

```typescript
// apps/web/src/app/api/ai-chat/route.ts
// Ya est√° implementado parcialmente (l√≠nea 819-820)

// ‚úÖ Analytics ya se ejecuta en background (bien hecho!)
const analyticsPromise = initializeAnalyticsAsync();

// PERO podr√≠amos optimizar m√°s:
const handleVoiceQuestion = async (question: string) => {
  try {
    // ‚úÖ Iniciar m√∫ltiples operaciones en paralelo
    const [aiResponse, /* otros procesos futuros */] = await Promise.all([
      // Llamada principal a IA
      fetch('/api/ai-chat', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          message: question,
          context: 'onboarding',
          conversationHistory: conversationHistory,
          userName: userName,
          pageContext: pageContext,
          language: 'es'
        }),
      }),
      // ‚úÖ Otras operaciones que no bloquean la respuesta
      // Por ejemplo: logging, prefetch de siguiente paso, etc.
    ]);

    const data = await aiResponse.json();

    // ‚úÖ Iniciar audio mientras se actualiza UI
    const audioPromise = speakText(data.response);

    // Actualizar historial (no esperar a que termine el audio)
    setConversationHistory(prev => [
      ...prev,
      { role: 'user', content: question },
      { role: 'assistant', content: data.response }
    ]);

    // Esperar audio solo si es necesario
    await audioPromise;

  } catch (error) {
    console.error('Error:', error);
  }
};
```

**Trabajo requerido**: 15 minutos ‚è∞

---

## üìä Resumen de Mejoras

### Impacto Estimado por Optimizaci√≥n

| Optimizaci√≥n | Prioridad | Esfuerzo | Reducci√≥n de Latencia | Complejidad |
|-------------|-----------|----------|----------------------|-------------|
| 1. Eliminar middleware | üî¥ ALTA | 5 min | 100-200ms | ‚≠ê F√°cil |
| 2. Streaming ElevenLabs | üî¥ ALTA | 30 min | 600-1000ms | ‚≠ê‚≠ê Media |
| 3. Optimizar GPT-4o-mini | üü° MEDIA | 15 min | 200-500ms | ‚≠ê F√°cil |
| 4. Cache de respuestas | üü¢ BAJA | 2 horas | 2000-3500ms* | ‚≠ê‚≠ê‚≠ê Alta |
| 5. Feedback visual | üü° MEDIA | 30 min | 0ms (UX) | ‚≠ê F√°cil |
| 6. Config ElevenLabs | üü° MEDIA | 5 min | 200-400ms | ‚≠ê F√°cil |
| 7. Paralelizaci√≥n | üî¥ ALTA | 15 min | 100-200ms | ‚≠ê‚≠ê Media |

\* Para preguntas comunes (60-70% de casos)

### Plan de Implementaci√≥n Recomendado

#### **Fase 1: Ganancias R√°pidas (1 hora)** üöÄ

1. ‚úÖ Eliminar middleware `/api/lia/onboarding-chat` (5 min)
2. ‚úÖ Optimizar configuraci√≥n ElevenLabs (5 min)
3. ‚úÖ Ajustar par√°metros GPT-4o-mini (15 min)
4. ‚úÖ Implementar paralelizaci√≥n (15 min)
5. ‚úÖ Agregar feedback visual inmediato (30 min)

**Reducci√≥n esperada**: 600-1300ms (15-25%)

---

#### **Fase 2: Optimizaci√≥n Profunda (30-60 min)** ‚ö°

6. ‚úÖ Implementar streaming de ElevenLabs (30-60 min)

**Reducci√≥n esperada adicional**: 600-1000ms (25-40%)

**Total acumulado**: 1200-2300ms (40-60%)

---

#### **Fase 3: Optimizaci√≥n Avanzada (2-3 horas)** üéØ

7. ‚úÖ Sistema de cach√© de respuestas predictivo (2-3 horas)

**Reducci√≥n para casos comunes**: 2000-3500ms adicionales

---

## üéØ Resultado Final Esperado

### Antes
```
Tiempo promedio: 3.5-6 segundos
Experiencia: "Lento y frustrante"
```

### Despu√©s (Fase 1 + 2)
```
Tiempo promedio: 1.5-2.5 segundos
Experiencia: "R√°pido y natural"
Reducci√≥n: 40-60%
```

### Despu√©s (Fase 1 + 2 + 3)
```
Tiempo promedio: 0.5-1 segundo (preguntas comunes)
Tiempo promedio: 1.5-2.5 segundos (preguntas nuevas)
Experiencia: "Casi instant√°neo"
Reducci√≥n: 60-85% (casos comunes)
```

---

## üîç Monitoreo y M√©tricas

### M√©tricas a Implementar

```typescript
// apps/web/src/core/components/OnboardingAgent/analytics.ts

interface VoiceMetrics {
  transcriptionTime: number;     // Web Speech API
  apiCallTime: number;            // Fetch a /api/ai-chat
  openaiProcessingTime: number;   // GPT-4o-mini
  ttsGenerationTime: number;      // ElevenLabs
  audioPlaybackTime: number;      // Reproducci√≥n
  totalTime: number;              // End-to-end
  cacheHit: boolean;              // ¬øUs√≥ cach√©?
}

function trackVoiceInteraction(metrics: VoiceMetrics) {
  console.log('üìä M√©tricas de interacci√≥n por voz:', metrics);

  // Enviar a analytics
  if (typeof window !== 'undefined' && window.gtag) {
    window.gtag('event', 'voice_interaction', {
      transcription_time: metrics.transcriptionTime,
      total_time: metrics.totalTime,
      cache_hit: metrics.cacheHit,
    });
  }

  // Log para debugging
  localStorage.setItem('last_voice_metrics', JSON.stringify(metrics));
}
```

### Dashboard de Monitoreo

Crear una vista en el admin panel que muestre:

- ‚è±Ô∏è Tiempo promedio de respuesta
- üìä Distribuci√≥n de latencias por componente
- üéØ Tasa de aciertos de cach√©
- üí∞ Costos de OpenAI y ElevenLabs
- üî• Preguntas m√°s frecuentes

---

## ‚úÖ Checklist de Implementaci√≥n

### Fase 1: Ganancias R√°pidas
- [ ] Eliminar `/api/lia/onboarding-chat` y usar `/api/ai-chat` directamente
- [ ] Cambiar `eleven_multilingual_v2` ‚Üí `eleven_turbo_v2_5`
- [ ] Ajustar `optimize_streaming_latency: 4`
- [ ] Ajustar `output_format: 'mp3_22050_32'`
- [ ] Reducir `max_tokens` a 150 para contexto onboarding
- [ ] Implementar sonido de confirmaci√≥n
- [ ] Mostrar placeholder "Pensando..." instant√°neamente
- [ ] Paralelizar operaciones no cr√≠ticas

### Fase 2: Streaming
- [ ] Implementar streaming de ElevenLabs con MediaSource
- [ ] Agregar fallback para navegadores sin soporte MediaSource
- [ ] Optimizar buffer de audio para inicio r√°pido
- [ ] Testing en m√∫ltiples navegadores

### Fase 3: Cach√©
- [ ] Crear sistema de cach√© de respuestas
- [ ] Implementar algoritmo de similitud de preguntas
- [ ] Precargar respuestas comunes al cargar componente
- [ ] Implementar invalidaci√≥n de cach√© (TTL: 7 d√≠as)
- [ ] UI para administrar cach√© en admin panel

### Monitoreo
- [ ] Implementar tracking de m√©tricas
- [ ] Crear dashboard de analytics
- [ ] Configurar alertas para latencias >3s
- [ ] A/B testing para validar mejoras

---

## üö® Consideraciones de Seguridad

### API Keys Hardcodeadas

**CR√çTICO**: La API key de ElevenLabs est√° hardcodeada en el frontend (l√≠nea 152):

```typescript
const apiKey = 'sk_dd0d1757269405cd26d5e22fb14c54d2f49c4019fd8e86d0';
```

**Riesgo**:
- ‚ùå La API key es visible en el c√≥digo del cliente
- ‚ùå Cualquiera puede usarla para consumir tu cr√©dito
- ‚ùå Posible abuso y costos inesperados

**Soluci√≥n Urgente**:

```typescript
// 1. Mover API key al backend
// apps/web/src/app/api/tts/route.ts (NUEVO ARCHIVO)

export async function POST(request: NextRequest) {
  const { text } = await request.json();

  const apiKey = process.env.ELEVENLABS_API_KEY; // ‚úÖ Seguro
  const voiceId = process.env.ELEVENLABS_VOICE_ID;

  const response = await fetch(
    `https://api.elevenlabs.io/v1/text-to-speech/${voiceId}/stream`,
    {
      method: 'POST',
      headers: {
        'Accept': 'audio/mpeg',
        'Content-Type': 'application/json',
        'xi-api-key': apiKey!,
      },
      body: JSON.stringify({
        text: text,
        model_id: 'eleven_turbo_v2_5',
        voice_settings: { /* ... */ }
      }),
    }
  );

  // Retornar stream directamente al cliente
  return new NextResponse(response.body, {
    headers: {
      'Content-Type': 'audio/mpeg',
    },
  });
}

// 2. Usar desde frontend
// apps/web/src/core/components/OnboardingAgent/OnboardingAgent.tsx

const speakText = async (text: string) => {
  const response = await fetch('/api/tts', { // ‚úÖ Proxy seguro
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({ text }),
  });

  const audioBlob = await response.blob();
  // ... resto del c√≥digo
};
```

**Prioridad**: üî¥ **URGENTE**

---

## üí° Optimizaciones Futuras

### Largo Plazo

1. **WebRTC para Audio Bidireccional**
   - Streaming bidireccional en tiempo real
   - Latencia ultra-baja (<500ms)
   - Complejidad: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê

2. **Edge Computing con Cloudflare Workers**
   - Ejecutar IA en el edge m√°s cercano al usuario
   - Reducci√≥n de latencia de red
   - Complejidad: ‚≠ê‚≠ê‚≠ê‚≠ê

3. **Modelos de IA On-Device**
   - Web LLM (LLaMA.cpp en WASM)
   - TTS on-device con SpeechSynthesis mejorado
   - Sin latencia de red
   - Complejidad: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê

4. **Predictive Pre-loading**
   - ML para predecir siguientes preguntas
   - Precargar respuestas antes de que se pregunten
   - Complejidad: ‚≠ê‚≠ê‚≠ê‚≠ê

---

## üìö Referencias T√©cnicas

### Documentaci√≥n ElevenLabs
- [Streaming API](https://elevenlabs.io/docs/api-reference/streaming)
- [Latency Optimization](https://elevenlabs.io/docs/api-reference/latency-optimization)
- [Voice Settings](https://elevenlabs.io/docs/api-reference/text-to-speech)

### Documentaci√≥n OpenAI
- [GPT-4o-mini](https://platform.openai.com/docs/models/gpt-4o-mini)
- [Chat Completions API](https://platform.openai.com/docs/guides/chat-completions)

### Web APIs
- [MediaSource API](https://developer.mozilla.org/en-US/docs/Web/API/MediaSource)
- [Web Speech API](https://developer.mozilla.org/en-US/docs/Web/API/Web_Speech_API)
- [Streams API](https://developer.mozilla.org/en-US/docs/Web/API/Streams_API)

---

## üéì Conclusiones

El agente de voz actual tiene una arquitectura s√≥lida pero con oportunidades significativas de optimizaci√≥n. Las mejoras propuestas pueden **reducir la latencia en 40-60%** con implementaciones relativamente sencillas, mejorando dram√°ticamente la experiencia del usuario.

**Recomendaci√≥n**: Comenzar con **Fase 1 + Fase 2** (1.5-2 horas de trabajo) para obtener el m√°ximo impacto con m√≠nimo esfuerzo. La **Fase 3** (cach√©) puede implementarse posteriormente como optimizaci√≥n adicional.

---

**Documento creado**: Noviembre 2025
**√öltima actualizaci√≥n**: Noviembre 2025
**Autor**: An√°lisis t√©cnico para optimizaci√≥n del agente de voz LIA
